{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "brown-vintage",
   "metadata": {},
   "source": [
    "# Welcome to the OBD SDD BE!\n",
    "\n",
    "## Adrien Delgado\n",
    "## Corentin Lefloch\n",
    "\n",
    "Today, the goal is to understand how a distributed system can be useful when dealing with medium to large scale data sets.  \n",
    "We'll see that Dask start to be nice as soon as the Data we need to process doesn't quite fit in memory, but also if we\n",
    "need to launch several computations in parallel.\n",
    "\n",
    "In this evaluation, you will:\n",
    "- Use Dask to read and understand the several gigabytes input dataset in a interactive way,\n",
    "- Preprocess the data in a distributed way: cleaning it up and adding some useful features,\n",
    "- Launch some model training that can be parallelized,\n",
    "- Reduce the dataset and train more accurate models on less Data,\n",
    "- Do an hyper parameter search to find the best model on a small sample of Data.\n",
    "\n",
    "In order to run and fill this notebook, you'll need to first deploy a Dask enabled Kubernetes cluster as seen last week. So please use the Kubernetes_DaskHub notebook for the steps to do it. __Be careful, it has been updated to use a Docker image containing ML Libraries since the last time!__.\n",
    "\n",
    "Once the Jupyterhub is up, you can clone the OBD directory from a Jupyterlab terminal to get this notebook, and select the default kernel.\n",
    "```\n",
    "git clone https://github.com/SupaeroDataScience/OBD.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-rugby",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "It is some statistics about NY Taxi cabs. \n",
    "\n",
    "See https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview, or https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data.\n",
    "        \n",
    "The goal of this evaluation will be to generate a model using machine learning technics that will predict the fare amount\n",
    "of a taxi ride given the other input parameters we have.\n",
    "\n",
    "The model will be evaluated using the Root mean squared error algorithm:  \n",
    "https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview/evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-navigator",
   "metadata": {},
   "source": [
    "## Try to analyze the Data using Kaggles'start-up code\n",
    "\n",
    "As an introduction, we'll use Kaggle starters'code to get some insights on the data set and\n",
    "computations we'll do and measure pandas library (non parallelized access) performance.\n",
    "\n",
    "See https://www.kaggle.com/dster/nyc-taxi-fare-starter-kernel-simple-linear-model where this comes from.\n",
    "\n",
    "On our data set (train and test available in gs://obd-dask), we'll see that with Kaggle method, we don't obtain a really good evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-jenny",
   "metadata": {},
   "source": [
    "#### Reading the data with pandas\n",
    "\n",
    "We're reading only about 20% from the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-halifax",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "train_df =  pd.read_csv('gs://obd-dask/train.csv', nrows = 10_000)\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-warehouse",
   "metadata": {},
   "source": [
    "#### Analysing dataset, adding some feature and droping null values\n",
    "\n",
    "Let's see if we can see some links between passenger count and fare amount?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_df.groupby(train_df.passenger_count).fare_amount.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-bridges",
   "metadata": {},
   "source": [
    "Maybe adding some feature about the distance of the trip could be a good idea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 'abs_diff_latitude' reprensenting the \"Manhattan vector\" from\n",
    "# the pickup location to the dropoff location.\n",
    "def add_travel_vector_features(df):\n",
    "    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n",
    "    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n",
    "\n",
    "add_travel_vector_features(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-stanley",
   "metadata": {},
   "source": [
    "Are there some undefined values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Old size: %d' % len(train_df))\n",
    "train_df = train_df.dropna(how = 'any', axis = 'rows')\n",
    "print('New size: %d' % len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-airline",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-developer",
   "metadata": {},
   "source": [
    "#### Quick analyze on new features and clean outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-mineral",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot = train_df.iloc[:2000].plot.scatter('abs_diff_longitude', 'abs_diff_latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Old size: %d' % len(train_df))\n",
    "train_df = train_df[(train_df.abs_diff_longitude < 5.0) & (train_df.abs_diff_latitude < 5.0)]\n",
    "print('New size: %d' % len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot = train_df.iloc[:2000].plot.scatter('abs_diff_longitude', 'abs_diff_latitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-paint",
   "metadata": {},
   "source": [
    "#### Get training features and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "# using the travel vector, plus a 1.0 for a constant bias term.\n",
    "def get_input_matrix(df):\n",
    "    return np.column_stack((df.abs_diff_longitude, df.abs_diff_latitude, np.ones(len(df))))\n",
    "\n",
    "train_X = get_input_matrix(train_df)\n",
    "train_y = np.array(train_df['fare_amount'])\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-feedback",
   "metadata": {},
   "source": [
    "#### Train a simple linear model using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The lstsq function returns several things, and we only care about the actual weight vector w.\n",
    "(w, _, _, _) = np.linalg.lstsq(train_X, train_y, rcond = None)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-irish",
   "metadata": {},
   "source": [
    "#### Make prediction on our test set and measure performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df =  pd.read_csv('gs://obd-dask/test.csv')\n",
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_travel_vector_features(test_df)\n",
    "test_X = get_input_matrix(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_predictions = np.matmul(test_X, w).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_ref = test_df.fare_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_y_ref, test_y_predictions, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-symphony",
   "metadata": {},
   "source": [
    "OK, so about 5,23$ of RMSE, this is not that bad... But we can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-hobby",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "### Some questions on this first Analysis\n",
    "\n",
    "- What is the most expensive part of the analysis, the one that takes the most time (see the %%time we used above)?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The most expensive part of the analysis was the data loading in the first cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-india",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Try to load the whole dataset with Pandas and comment.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train_df_full =  pd.read_csv('gs://obd-dask/train.csv')\n",
    "## My kernel died... How fun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-plaza",
   "metadata": {},
   "source": [
    "# Processing our data set using dask\n",
    "\n",
    "Dask will help us processing all the input data set. It is really useful when input data is too big to fit in memory. In this case, it can stream the computation by data blocs one one computer, or distribute the computation on several computers.\n",
    "\n",
    "This is what we'll do next!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-shame",
   "metadata": {},
   "source": [
    "### Start an appropriate sized Dask cluster for our analysis\n",
    "\n",
    "We'll need a Dask cluster to pre process the data and distribute some learning, the following code starts one in our K8S infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "# Use values stored in your local configuration (recommended)\n",
    "gateway = Gateway()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "gateway.list_clusters()\n",
    "#cluster = gateway.connect('daskhub.5c7ed511372b4367a324a9eab59794a5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = gateway.new_cluster(worker_cores=1, worker_memory=3.4)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-mumbai",
   "metadata": {},
   "source": [
    "__Please click on the Dashboard link above, it will help you a lot!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cluster.get_client()\n",
    "cluster.scale(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-hometown",
   "metadata": {},
   "source": [
    "### Launch some computation, what about Pi ?\n",
    "\n",
    "Just to check our cluster is working!\n",
    "\n",
    "We'll use Dask array, a Numpy extension for this, taht we'll use later on for the Machine Learning part of this evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import dask.array as da\n",
    "\n",
    "sample = 10_000_000_000  # <- this is huge!\n",
    "xxyy = da.random.uniform(-1, 1, size=(2, sample))\n",
    "norm = da.linalg.norm(xxyy, axis=0)\n",
    "summ = da.sum(norm <= 1)\n",
    "insiders = summ.compute()\n",
    "pi = 4 * insiders / sample\n",
    "print(\"pi ~= {}\".format(pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-bathroom",
   "metadata": {},
   "source": [
    "## Now, access the data of our BE using Dask\n",
    "\n",
    "We'll use Dask Dataframe, an distributed version of Pandas Dataframe.\n",
    "\n",
    "See https://docs.dask.org/en/latest/dataframe.html.\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "So instead of using Pandas to load the dataset, just use the equivalent dask method from dask.dataframe.\n",
    "\n",
    "- Fill the following cell with the appropriate code to read the data using Dask.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = dd.read_csv('gs://obd-dask/train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-millennium",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "### Some questions about this data loading\n",
    "\n",
    "- That was fast for several gigabytes, wasn't it? Why is this, what did we do?\n",
    "- Why the return dataframe looks empty?\n",
    "- See the number of partitions described above? What does it correspond to?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dask only reads the first line of the dataset to get the name of the columns and the types \n",
    "## The dataframe looks empty because it was not been read yet\n",
    "## 85 partitions for the whole dataset. It corresponds to the number of sub-dataframes to load in order parallelize the read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-greek",
   "metadata": {},
   "source": [
    "## Little warm up: Analyzing our data to better understand it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-pierre",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- First, how many records do we have? (hint, in python, len() works for almost any object).\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-parts",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- What did happend when counting record of our Dask dataframe? Remember with the Spark tutorial: transformations and actions... Same kind of concepts exist in Dask. Just look at the Dask Dashboard!\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For each chunk, dask reads the dataset, then gets the length of it, and sums the length to previously calculated ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-calculator",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Compare the time of this computation to the time of loading a subset of the Dataset with Pandas. What is fast enough considering the number of worker we have?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Answer needed there.\n",
    "n_rows = 54869617//85\n",
    "\n",
    "train_df =  pd.read_csv('gs://obd-dask/train.csv', nrows = n_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-diving",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.9 seconds * 85 is way more than 16 seconds! Dask is efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-withdrawal",
   "metadata": {},
   "source": [
    "Let's have a look at some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-headline",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Why was it faster than the count records operation above? What did wee read?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We only opened the 5 first lines, instead of reading everything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-bennett",
   "metadata": {},
   "source": [
    "df = dd.read_csv('gs://obd-dask/train.csv')<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Let's compute the mean of the fare given the passenger_count, as we've done with Pandas. Please fill the blank. (hint: don't forget the compute() call)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv('gs://obd-dask/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-honey",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.groupby('passenger_count').fare_amount.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-institute",
   "metadata": {},
   "source": [
    "Wow, ever seen a cab with more than 200 people??\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- This is a bit slow, much more than with Pandas, why? Which part of the computation is slow, look at the Dashboard to see the name of the tasks. \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the data is the longest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-tradition",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- How cloud we optimize the next computations ? Where will be the data at the end?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-security",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Look at the Dashboard at what is happening beind the scene.\n",
    "At the end, try again the computation:\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.groupby('passenger_count').fare_amount.mean().compute()\n",
    "## Answer needed here, the same computation on fare_amount mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-stone",
   "metadata": {},
   "source": [
    "Much better isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-count",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "### Some other questions to practice\n",
    "\n",
    "- Can you see a correlation between the fare amount and the dropoff latitude? Answer by doing a dask dataframe computation.\n",
    "\n",
    "First you'll need to round the dropoff latitude to have some sort of categories using Series.round() function.\n",
    "\n",
    "Then, just group_by this new colon to have some answer (and don't forget to compute to get the results).\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.round({'dropoff_latitude' : 0}).groupby('dropoff_latitude').fare_amount.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-upset",
   "metadata": {},
   "source": [
    "OK, this don't give a lot of insights, but it looks like we've got some strange values somewhere!\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- Let's just have a look of non extreme values, so probably some records at the middle of the results.\n",
    "We need first to sort the resulting series by index befor lookin at the middle of it.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_by_lat = df.round({'dropoff_latitude' : 1}).groupby('dropoff_latitude').fare_amount.mean().compute().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-petersburg",
   "metadata": {},
   "source": [
    "OK, this is not really useful, but it's an exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_by_lat.iloc[550:580]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-index",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Do you think we could parallelize things better for any of our computation or data access?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Maybe, maybe not, we are not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-people",
   "metadata": {},
   "source": [
    "## Let's do some preprocessing of our data to clean it up and add some features\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- You'll need to do the same operations as in pandas, we just need to call compute when needing a result, and not compute when building our dataframe transformations.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-debate",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "#### Cleaning up\n",
    "\n",
    "- Is there some null values in our data?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-highland",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-clear",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Yep! We must get rid of them...\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-health",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Old size: %d' % len(df))\n",
    "df = df.dropna()\n",
    "print('New size: %d' % len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-matter",
   "metadata": {},
   "source": [
    "#### Adding features\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- As with Pandas above, add the lattitude and longitude distance vector with a function call\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-basis",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "add_travel_vector_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-rouge",
   "metadata": {},
   "source": [
    "A quick look at our Dataframe to check things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-funeral",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Now let's quickly plot a subset of our travel vector features to see its distribution. Use dask.dataframe.sample() to get about five percent of the rows, and get it back with compute and plot like with Pandas\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(frac=0.05).compute().plot.scatter('abs_diff_longitude', 'abs_diff_latitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-maryland",
   "metadata": {},
   "source": [
    "Wow, looks like we have some strange values here: more than 1000Â° of distance... There's a problem somewhere.\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Just get rid of the extreme values, we should keep inside the city wall or so. Like with Pandas.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Old size: %d' % len(df))\n",
    "df = df[(df.abs_diff_longitude < 5.0) & (df.abs_diff_latitude < 5.0)]\n",
    "print('New size: %d' % len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-sydney",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- What is triggering the computation in the examples above?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-falls",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the .compute() is triggering the computation in the examples. \n",
    "## Before that, it is only staging computations without actually running them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-landing",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- you can do another plot like above with the filtered values if you like.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(frac=0.05).compute().plot.scatter('abs_diff_longitude', 'abs_diff_latitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-commercial",
   "metadata": {},
   "source": [
    "Ok, let's see some statistics on our Dataset. The describe() function inherited from Pandas compute a lot of statistics on a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-luxembourg",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df.describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-sharp",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Are there some values that still looks odd to you in here?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 93k$ in fare, -300$, logitudes of thousands, same for longitudes, 208 passengers..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-scanning",
   "metadata": {},
   "source": [
    "## Training a model in a distributed way\n",
    "\n",
    "Let's begin with a linear model that we can distributed with Dask ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-toner",
   "metadata": {},
   "source": [
    "### Building our feature vectors\n",
    "\n",
    "Here again define a method so that we can use it later for our test set evaluation.\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Just do the same as with the Pandas example by defining a get_input_matrix(df) function. But this time you'll generate a dask array using to_dask_array(length=True) method on the dataframe. You should do a method that generate the X input features dask array, and also the same with y training results. You can do just one method that return both. \n",
    "- It is a good idea to persist() arrays in memory in or after the call.\n",
    "- This time, we'll add the feature 'passenger_count' in addition to the distance vectors.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-sherman",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df # rename df for understanding purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "# using the travel vector, and the passenger count.\n",
    "def get_input_matrix(df):\n",
    "    return df[[\"abs_diff_longitude\", \"abs_diff_latitude\", \"passenger_count\"]].to_dask_array(lengths=True), df.fare_amount.to_dask_array(lengths=True)\n",
    "\n",
    "train_X, train_y = get_input_matrix(train_df)\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-geometry",
   "metadata": {},
   "source": [
    "Then we get the values, and display train_X to have some insights of its size and chunking scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-genetics",
   "metadata": {},
   "source": [
    "### Distributed training a Linear model\n",
    "\n",
    "Be careful, this can take time, try first with few iterations (max_iter = 5).\n",
    "\n",
    "see https://ml.dask.org/glm.html  \n",
    "and https://ml.dask.org/modules/generated/dask_ml.linear_model.LinearRegression.html#dask_ml.linear_model.LinearRegression\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Train a LinearRegression model from dask_ml.linear_model on our inputs\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(max_iter=5)\n",
    "lr.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-suite",
   "metadata": {},
   "source": [
    "## Evaluating our model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-religious",
   "metadata": {},
   "source": [
    "#### First we should load the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = dd.read_csv('gs://obd-dask/test.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-opinion",
   "metadata": {},
   "source": [
    "Adding our features to the test set and getting our feature array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_travel_vector_features(test_df)\n",
    "test_X, test_y = get_input_matrix(test_df)\n",
    "test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-formation",
   "metadata": {},
   "source": [
    "We can use the score method inherited from Scikit learn, it gives some hints on the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-secretariat",
   "metadata": {},
   "source": [
    "Just get the numpy arrays for computing final score, this is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_X.compute()\n",
    "test_y = test_y.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-frost",
   "metadata": {},
   "source": [
    "#### Compute the RMSE\n",
    "\n",
    "https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_y, lr.predict(test_X), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-marathon",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- What RMSE did you get? Compare it to the Pandas only computation.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Around 20 cents better than with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-field",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Why is this model not really good?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Because we are using a linear model on data which has no apparent reason of having linear correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-alpha",
   "metadata": {},
   "source": [
    "## Use Dask to scale computation on Hyper Parameter Search\n",
    "\n",
    "As seen above, Dask is well suited to distribute Data and learn a model on a big Data set. However, not all the models can be train in parallel on sub chunks of Data. See https://scikit-learn.org/stable/computing/scaling_strategies.html for the compatible models of Sickit learn for example.\n",
    "\n",
    "Dask can also be used to train several model in parallel on small datasets, this is what we'll try now.\n",
    "\n",
    "We will just take a sample of the training set, and try to learn several models with different hyper parameters, and find the best one.\n",
    "\n",
    "Dask Hyper parameter search : https://ml.dask.org/hyper-parameter-search.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-shareware",
   "metadata": {},
   "source": [
    "First we'll take a small subset of the Data, 5% is a maximum if we want to avoir memory issues on our workers and have appropriate training times. You can try with less if the results are still good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-notion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a sample of the input data, get it as pandas dataframe\n",
    "train_sample_df = train_df.sample(frac=0.05, random_state=123456)\n",
    "# Get feature vectors out of it\n",
    "train_sample_X, train_sample_y = get_input_matrix(train_sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-loading",
   "metadata": {},
   "source": [
    "In order to optimize things, we can also change the type of the features to more appropriate and small types.\n",
    "\n",
    "We also need to use Numpy arrays, so we'll gather the result from Dask to local variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_X = train_sample_X.astype('float32').compute()\n",
    "train_sample_y = train_sample_y.astype('float32').compute()\n",
    "train_sample_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-ending",
   "metadata": {},
   "source": [
    "What size is our dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.getsizeof(train_sample_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-synthesis",
   "metadata": {},
   "source": [
    "About 32MB, this is still quite a big dataset for standard machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-williams",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- Now, just use dask hyper parameter search Dask API to distribute the search. You can either use joblib integration with Sklearn or dask_ml directly. Be careful: do not use model too long to train, and limit their complexity at first or the combinations of hyper parameters you'll use. Hint, start first with a simple LinearModel like SGDRegressor and not more than 10 iterations per model.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-messaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from dask_ml.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sgd = SGDRegressor(max_iter=10)\n",
    "params = {'alpha': [0.001, 0.01, 0.1, 0.5, 0.99],\n",
    "          'penalty': ['l2', 'l1', 'elasticnet']\n",
    "         }\n",
    "search = GridSearchCV(sgd, params)\n",
    "search.fit(train_sample_X, train_sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_y, search.predict(test_X), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-scroll",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- So how does the result compare to distributed leaning with a linear model? On all the dataset?\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It is about the same so far, but as it is with only 5% on the data, \n",
    "## we think we can consider it is not too bad compared to the linear regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-subdivision",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Can you do better with Random forest? Caution: use limited trees, small number of estimators < 5 and max_depth < 40...\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rfr = RandomForestRegressor(n_estimators=5, max_depth=30)\n",
    "params = {\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'min_impurity_decrease': [0.0, 0.01]\n",
    "}\n",
    "search = GridSearchCV(rfr, params)\n",
    "search.fit(train_sample_X, train_sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-significance",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- What do you observe when training RandomForest tree on Dask parallelization Dashboard? Can you explain why there are so many tasks?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-vaccine",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-arbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_y, search.predict(test_X), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-eight",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Did you get better results with RandomForest? Why ?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We got better results with randomForest than with linear regression because the data \n",
    "# we try to analyse does not necessarily imply linear correlations between its features,\n",
    "# hence the poor performance of the LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-style",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "# Extend this notebook\n",
    "    \n",
    "Try to do better!\n",
    "\n",
    "- Add new features to the input Data using Dask Dataframes, or clean it better. Reapply the learning above with these new features. Do you get better results? Some suggestions for a better leaning:\n",
    "  - Max passenger count of 208, maybe we should ignore this value? Rides with 0 passengers? Try to drop some data.\n",
    "  - Apply some normalisation or regularization or other feature transformation? See https://ml.dask.org/preprocessing.html.\n",
    "  - There are 0m rides?\n",
    "  - Negative fare amount?? Drop some data.\n",
    "  - Maybe the hour of the day, or the month, has some impact on fares? Try to add features. See https://matthewrocklin.com/blog/work/2017/01/12/dask-dataframes for some hints on how to do this.\n",
    "  - Maybe try to find a way to use the start and drop off locations?\n",
    "- Improve the model parameters or find a better one. Try using this time dask_ml HyperbandSearchCV. See https://ml.dask.org/hyper-parameter-search.html#basic-use. You can use it for example with https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor.\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning a bit the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean fare_amount\n",
    "print('Old size: %d' % len(train_df))\n",
    "train_df = train_df[(train_df.fare_amount > 0.0) & (train_df.fare_amount < 1000)]\n",
    "print('New size: %d' % len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean passenger_count\n",
    "print('Old size: %d' % len(train_df))\n",
    "train_df = train_df[(train_df.passenger_count > 0) & (train_df.passenger_count < 20)]\n",
    "print('New size: %d' % len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean ride length\n",
    "print('Old size: %d' % len(train_df))\n",
    "train_df = train_df[(train_df.abs_diff_longitude > 0) | (train_df.abs_diff_latitude > 0)]\n",
    "print('New size: %d' % len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try to add the pickup hour and month\n",
    "\n",
    "def get_month(datetime):\n",
    "    return int(datetime[5:7])\n",
    "\n",
    "def get_hour(datetime):\n",
    "    return int(datetime[11:13])\n",
    "\n",
    "def add_datetime_features(df):\n",
    "    df['pickup_month'] = df.pickup_datetime.apply(lambda datetime: int(datetime[5:7]), meta=int)\n",
    "    df['pickup_hour'] = df.pickup_datetime.apply(lambda datetime: int(datetime[11:13]), meta=int)\n",
    "\n",
    "add_datetime_features(train_df)\n",
    "add_datetime_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the travel vector, the pickup month and hour and the passenger count.\n",
    "def get_new_input_matrix(df):\n",
    "    return df[[\"abs_diff_longitude\", \"abs_diff_latitude\", \"passenger_count\", \"pickup_month\", \"pickup_hour\"]].to_dask_array(lengths=True), df.fare_amount.to_dask_array(lengths=True)\n",
    "\n",
    "train_X, train_y = get_new_input_matrix(train_df)\n",
    "test_X, test_y = get_new_input_matrix(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-stocks",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a sample of the input data, get it as pandas dataframe\n",
    "train_sample_df = train_df.sample(frac=0.05, random_state=123456)\n",
    "# Get feature vectors out of it\n",
    "train_sample_X, train_sample_y = get_new_input_matrix(train_sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-finnish",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_X = train_sample_X.persist()\n",
    "train_sample_X = train_sample_X.astype('float32').compute()\n",
    "train_sample_y = train_sample_y.astype('float32').compute()\n",
    "train_sample_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Let's try again the same RandomForestRegressor with the new features and cleaned dataset\n",
    "\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=5, max_depth=30)\n",
    "params = {\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'min_impurity_decrease': [0.0, 0.01]\n",
    "}\n",
    "search = GridSearchCV(rfr, params)\n",
    "search.fit(train_sample_X, train_sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_y, search.predict(test_X), squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-ocean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For an unknown reason, the RandomForestRegressor performs worse once the data is cleaned..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import HyperbandSearchCV\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mlp = MLPRegressor(hidden_layer_sizes=100, max_iter=100)\n",
    "params = {\n",
    "    'alpha': np.logspace(1e-4,1.0, num=5),\n",
    "    'learning_rate_init': np.logspace(0.001, 0.1, num=3),\n",
    "#     'solver': ['sgd', 'adam'],\n",
    "#     'learning_rate': ['constant', 'adaptive', 'invscaling'],\n",
    "}\n",
    "search = HyperbandSearchCV(mlp, params, max_iter=10)\n",
    "search.fit(train_sample_X, train_sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_y, search.predict(test_X), squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The neural network does not seem to perform better than the RandomForestRegressor in that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-appeal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the end, the best result we got was with the RandomForestRegressor, before cleaning the data and adding the hour,\n",
    "# and we had an error of around $3.58"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
